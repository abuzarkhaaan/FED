{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9271850,"sourceType":"datasetVersion","datasetId":5611070},{"sourceId":9272847,"sourceType":"datasetVersion","datasetId":5611825},{"sourceId":9272929,"sourceType":"datasetVersion","datasetId":5611886},{"sourceId":9273018,"sourceType":"datasetVersion","datasetId":5611957},{"sourceId":76738,"sourceType":"modelInstanceVersion","modelInstanceId":64502,"modelId":88752}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q /kaggle/input/emotion12121212 -d /kaggle/working/emotion_dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:07:56.396615Z","iopub.execute_input":"2024-08-30T04:07:56.397064Z","iopub.status.idle":"2024-08-30T04:07:57.384637Z","shell.execute_reply.started":"2024-08-30T04:07:56.397027Z","shell.execute_reply":"2024-08-30T04:07:57.383530Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"unzip:  cannot find or open /kaggle/input/emotion12121212, /kaggle/input/emotion12121212.zip or /kaggle/input/emotion12121212.ZIP.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:07:57.386222Z","iopub.execute_input":"2024-08-30T04:07:57.386558Z","iopub.status.idle":"2024-08-30T04:07:58.386711Z","shell.execute_reply.started":"2024-08-30T04:07:57.386523Z","shell.execute_reply":"2024-08-30T04:07:58.385723Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"emotion12121212  fsafsdf  helloo  videotest  yolo-v8\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/emotion12121212/\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:07:58.389424Z","iopub.execute_input":"2024-08-30T04:07:58.389791Z","iopub.status.idle":"2024-08-30T04:07:59.364958Z","shell.execute_reply.started":"2024-08-30T04:07:58.389729Z","shell.execute_reply":"2024-08-30T04:07:59.363990Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"README.dataset.txt  README.roboflow.txt  data.yaml  test  train  valid\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip -q /kaggle/input/emotion12121212/emotion12121212.zip -d /kaggle/working/emotion_dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:07:59.366346Z","iopub.execute_input":"2024-08-30T04:07:59.366651Z","iopub.status.idle":"2024-08-30T04:08:00.350387Z","shell.execute_reply.started":"2024-08-30T04:07:59.366617Z","shell.execute_reply":"2024-08-30T04:08:00.349412Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"unzip:  cannot find or open /kaggle/input/emotion12121212/emotion12121212.zip, /kaggle/input/emotion12121212/emotion12121212.zip.zip or /kaggle/input/emotion12121212/emotion12121212.zip.ZIP.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/emotion12121212/\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:08:00.351889Z","iopub.execute_input":"2024-08-30T04:08:00.352220Z","iopub.status.idle":"2024-08-30T04:08:01.334635Z","shell.execute_reply.started":"2024-08-30T04:08:00.352184Z","shell.execute_reply":"2024-08-30T04:08:01.333661Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"README.dataset.txt  README.roboflow.txt  data.yaml  test  train  valid\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/emotion12121212/\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:08:01.336128Z","iopub.execute_input":"2024-08-30T04:08:01.336454Z","iopub.status.idle":"2024-08-30T04:08:02.320777Z","shell.execute_reply.started":"2024-08-30T04:08:01.336420Z","shell.execute_reply":"2024-08-30T04:08:02.319775Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"README.dataset.txt  README.roboflow.txt  data.yaml  test  train  valid\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:08:02.322376Z","iopub.execute_input":"2024-08-30T04:08:02.322726Z","iopub.status.idle":"2024-08-30T04:08:03.313321Z","shell.execute_reply.started":"2024-08-30T04:08:02.322690Z","shell.execute_reply":"2024-08-30T04:08:03.312154Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"emotion12121212  fsafsdf  helloo  videotest  yolo-v8\n","output_type":"stream"}]},{"cell_type":"code","source":"# List the files inside the folder to confirm they are accessible\n!ls /kaggle/input/emotion12121212/\n\n# Example: If it's an image dataset, you can load an image using:\nimport os\nfrom PIL import Image\n\nimage_path = '/kaggle/input/emotion12121212/example_image.jpg'\nimg = Image.open(image_path)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/emotion12121212/\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:08:04.384550Z","iopub.status.idle":"2024-08-30T04:08:04.384914Z","shell.execute_reply.started":"2024-08-30T04:08:04.384725Z","shell.execute_reply":"2024-08-30T04:08:04.384744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust the path based on what you find\nimage_path = '/kaggle/input/emotion12121212/images/example_image.jpg'\nimg = Image.open(image_path)\nimg.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:08:04.386152Z","iopub.status.idle":"2024-08-30T04:08:04.386510Z","shell.execute_reply.started":"2024-08-30T04:08:04.386332Z","shell.execute_reply":"2024-08-30T04:08:04.386350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install the ultralytics package, which includes YOLOv8\n!pip install ultralytics\n\n# Install other essential libraries if not already installed\n!pip install torch torchvision\n!pip install opencv-python-headless\n!pip install matplotlib\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO\nimport matplotlib.pyplot as plt\n\n# Initialize the YOLO model (starting from a pretrained model for fine-tuning)\nmodel = YOLO('yolov8n.pt')  # yolov8n.pt is a small model, you can choose larger versions like yolov8m.pt, yolov8l.pt, etc.\n# Define the path to your dataset and the number of epochs\ndata_path = '/kaggle/input/emotion12121212/data.yaml'  # Update this to your dataset path\nepochs = 50  # Number of training epochs\n\n# Train the model\nmodel.train(data=data_path, epochs=epochs, imgsz=640, batch=16, project='emotion_detection', name='yolov8n', verbose=True)\n\n# Evaluate the model\nmetrics = model.val()\n\n# Print metrics\nprint(f\"Precision: {metrics['precision']}\")\nprint(f\"Recall: {metrics['recall']}\")\nprint(f\"mAP@0.5: {metrics['map']}\")\nprint(f\"mAP@0.5:0.95: {metrics['map50']}\")\n\n# Plot training results (loss, accuracy, etc.)\nresults = model.results\nresults.plot()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T04:09:37.202994Z","iopub.execute_input":"2024-08-30T04:09:37.203403Z","iopub.status.idle":"2024-08-30T05:51:49.483122Z","shell.execute_reply.started":"2024-08-30T04:09:37.203356Z","shell.execute_reply":"2024-08-30T05:51:49.481147Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|| 6.25M/6.25M [00:00<00:00, 105MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Ultralytics YOLOv8.2.83  Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/input/emotion12121212/data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=emotion_detection, name=yolov8n, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=emotion_detection/yolov8n\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|| 755k/755k [00:00<00:00, 22.9MB/s]\n2024-08-30 04:09:43,696\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-08-30 04:09:44,230\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=4\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \nModel summary: 225 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir emotion_detection/yolov8n', view at http://localhost:6006/\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240830_041051-e7d0qg5e</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection/runs/e7d0qg5e' target=\"_blank\">yolov8n</a></strong> to <a href='https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection' target=\"_blank\">https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection/runs/e7d0qg5e' target=\"_blank\">https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection/runs/e7d0qg5e</a>"},"metadata":{}},{"name":"stdout","text":"Freezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:268: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/emotion12121212/train/labels... 9485 images, 6 backgrounds, 0 corrupt: 100%|| 9485/9485 [00:44<00:00, 214.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING 锔 Cache directory /kaggle/input/emotion12121212/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/emotion12121212/valid/labels... 892 images, 0 backgrounds, 0 corrupt: 100%|| 892/892 [00:04<00:00, 192.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING 锔 /kaggle/input/emotion12121212/valid/images/images-2020-11-06T004041-046_face_png.rf.356e9608a7ae873d0819895236165983.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mval: \u001b[0mWARNING 锔 Cache directory /kaggle/input/emotion12121212/valid is not writeable, cache not saved.\nPlotting labels to emotion_detection/yolov8n/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1memotion_detection/yolov8n\u001b[0m\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/50      2.37G       1.15      2.082       1.44         35        640: 100%|| 593/593 [02:02<00:00,  4.85it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:06<00:00,  4.19it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.661      0.648      0.707      0.351\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/50      2.32G      1.095      1.389      1.381         43        640: 100%|| 593/593 [01:55<00:00,  5.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948       0.61      0.694      0.739      0.426\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/50      2.31G      1.055      1.218      1.346         35        640: 100%|| 593/593 [01:53<00:00,  5.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.773      0.758      0.824      0.457\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/50      2.31G      1.019      1.107      1.313         25        640: 100%|| 593/593 [01:52<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.714      0.696      0.744      0.421\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/50      2.31G     0.9816      1.028      1.283         34        640: 100%|| 593/593 [01:52<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.789      0.791      0.864      0.525\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/50      2.32G     0.9321     0.9487      1.251         31        640: 100%|| 593/593 [01:53<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.752      0.737      0.795      0.477\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/50      2.32G     0.8978     0.9031      1.229         35        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.791      0.802      0.879      0.527\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/50      2.31G     0.8831     0.8566      1.214         55        640: 100%|| 593/593 [01:53<00:00,  5.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.832       0.79      0.866      0.545\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/50      2.31G     0.8664     0.8269      1.202         25        640: 100%|| 593/593 [01:53<00:00,  5.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.835      0.844      0.895      0.551\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/50      2.32G     0.8577     0.8079      1.195         24        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.862      0.825       0.89       0.56\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      11/50      2.32G     0.8372     0.7618      1.183         28        640: 100%|| 593/593 [01:52<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.828      0.845      0.891      0.564\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      12/50      2.31G     0.8173     0.7449      1.173         28        640: 100%|| 593/593 [01:53<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.834      0.837      0.896      0.562\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      13/50      2.31G     0.8174     0.7297       1.17         36        640: 100%|| 593/593 [01:52<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.52it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.885      0.845      0.902      0.567\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      14/50      2.32G     0.8033     0.7115      1.164         34        640: 100%|| 593/593 [01:52<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.53it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.857      0.842      0.897      0.558\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      15/50      2.32G     0.7885     0.6861      1.157         45        640: 100%|| 593/593 [01:53<00:00,  5.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.858      0.864      0.911      0.575\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      16/50      2.31G     0.7814     0.6751      1.145         27        640: 100%|| 593/593 [01:52<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.18it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.906      0.858      0.909      0.572\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      17/50      2.31G     0.7732     0.6637      1.139         27        640: 100%|| 593/593 [01:52<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.893      0.863      0.918      0.577\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      18/50      2.32G     0.7606     0.6393      1.133         31        640: 100%|| 593/593 [01:52<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.878      0.882      0.922      0.592\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      19/50      2.32G     0.7503     0.6359      1.126         46        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.32it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.887       0.88      0.918      0.586\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      20/50      2.31G     0.7549     0.6252      1.128         38        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.879      0.886      0.924      0.581\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      21/50      2.31G     0.7388     0.6108      1.116         30        640: 100%|| 593/593 [01:52<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.892      0.887      0.925      0.581\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      22/50      2.32G     0.7338     0.6006      1.113         32        640: 100%|| 593/593 [01:52<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.882      0.879      0.925      0.591\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      23/50      2.32G     0.7278     0.5941       1.11         29        640: 100%|| 593/593 [01:51<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.82it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.856      0.877      0.914      0.595\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      24/50      2.31G     0.7196     0.5778      1.101         38        640: 100%|| 593/593 [01:51<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.907      0.878      0.931      0.595\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      25/50      2.31G     0.7035     0.5636      1.094         30        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.893      0.884      0.925      0.598\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      26/50      2.32G     0.7021     0.5578      1.094         31        640: 100%|| 593/593 [01:51<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.911      0.873       0.93      0.588\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      27/50      2.32G     0.6961      0.548      1.088         27        640: 100%|| 593/593 [01:52<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.888      0.885      0.921      0.591\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      28/50      2.31G     0.6924      0.538      1.089         28        640: 100%|| 593/593 [01:51<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.42it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.893      0.892      0.929      0.596\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      29/50      2.31G     0.6779     0.5248      1.078         30        640: 100%|| 593/593 [01:51<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.901      0.882      0.925      0.588\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      30/50      2.32G     0.6829     0.5306      1.082         31        640: 100%|| 593/593 [01:51<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.896      0.899      0.934      0.602\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      31/50      2.32G     0.6675     0.5074      1.074         33        640: 100%|| 593/593 [01:51<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.42it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.898      0.874      0.926      0.595\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      32/50      2.31G     0.6649     0.5049      1.073         32        640: 100%|| 593/593 [01:51<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.878      0.895      0.925      0.597\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      33/50      2.31G     0.6593     0.4944      1.068         34        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.885      0.889       0.92      0.589\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      34/50      2.32G     0.6434     0.4938      1.059         36        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.906      0.883      0.933      0.599\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      35/50      2.32G     0.6411     0.4745      1.056         40        640: 100%|| 593/593 [01:52<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.899      0.894      0.927      0.601\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      36/50      2.31G     0.6377     0.4785      1.052         35        640: 100%|| 593/593 [01:52<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.908      0.891      0.936      0.604\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      37/50      2.31G     0.6324     0.4705      1.051         20        640: 100%|| 593/593 [01:51<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948       0.92      0.883      0.931      0.602\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      38/50      2.32G      0.625     0.4561      1.047         32        640: 100%|| 593/593 [01:51<00:00,  5.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.893        0.9      0.933      0.599\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      39/50      2.32G     0.6189     0.4603      1.041         27        640: 100%|| 593/593 [01:51<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.915      0.888      0.933      0.607\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      40/50      2.31G     0.6112     0.4483      1.039         28        640: 100%|| 593/593 [01:51<00:00,  5.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.904      0.899      0.938      0.601\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      41/50      2.32G      0.627     0.3354      1.044         15        640: 100%|| 593/593 [01:52<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.82it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.911      0.886      0.938      0.599\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      42/50      2.32G     0.6022     0.3139       1.03         15        640: 100%|| 593/593 [01:50<00:00,  5.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.912      0.892      0.939      0.607\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      43/50      2.33G     0.5935     0.3098       1.02         14        640: 100%|| 593/593 [01:50<00:00,  5.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.899      0.905      0.935      0.599\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      44/50      2.32G     0.5854     0.3016      1.018         18        640: 100%|| 593/593 [01:50<00:00,  5.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.923      0.889      0.934      0.598\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      45/50      2.31G     0.5741     0.2917      1.007         14        640: 100%|| 593/593 [01:50<00:00,  5.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948       0.91      0.896      0.933        0.6\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      46/50      2.33G     0.5624     0.2823      1.003         13        640: 100%|| 593/593 [01:50<00:00,  5.39it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.917      0.892      0.931      0.599\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      47/50      2.33G      0.555     0.2795      0.999         13        640: 100%|| 593/593 [01:50<00:00,  5.38it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.923      0.898      0.936      0.603\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      48/50      2.31G     0.5468     0.2732      0.992         18        640: 100%|| 593/593 [01:49<00:00,  5.39it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.80it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.913      0.912      0.939      0.605\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      49/50      2.31G     0.5389     0.2699     0.9851         13        640: 100%|| 593/593 [01:50<00:00,  5.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:04<00:00,  5.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.921      0.898      0.932      0.599\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      50/50      2.32G     0.5323     0.2653     0.9869         13        640: 100%|| 593/593 [01:50<00:00,  5.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:05<00:00,  5.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.921      0.895      0.931        0.6\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n50 epochs completed in 1.647 hours.\nOptimizer stripped from emotion_detection/yolov8n/weights/last.pt, 6.2MB\nOptimizer stripped from emotion_detection/yolov8n/weights/best.pt, 6.2MB\n\nValidating emotion_detection/yolov8n/weights/best.pt...\nUltralytics YOLOv8.2.83  Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 168 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 28/28 [00:06<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.911      0.893      0.939      0.607\n                 angry        207        213      0.924      0.892      0.954      0.836\n                 happy        246        283      0.903      0.888      0.927      0.447\n                   sad        232        233      0.895      0.884      0.939      0.562\n             surprised        207        219       0.92      0.909      0.936      0.581\nSpeed: 0.1ms preprocess, 1.6ms inference, 0.0ms loss, 2.5ms postprocess per image\nResults saved to \u001b[1memotion_detection/yolov8n\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='12.571 MB of 12.571 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td></td></tr><tr><td>lr/pg1</td><td></td></tr><tr><td>lr/pg2</td><td></td></tr><tr><td>metrics/mAP50(B)</td><td></td></tr><tr><td>metrics/mAP50-95(B)</td><td></td></tr><tr><td>metrics/precision(B)</td><td></td></tr><tr><td>metrics/recall(B)</td><td></td></tr><tr><td>model/GFLOPs</td><td></td></tr><tr><td>model/parameters</td><td></td></tr><tr><td>model/speed_PyTorch(ms)</td><td></td></tr><tr><td>train/box_loss</td><td></td></tr><tr><td>train/cls_loss</td><td></td></tr><tr><td>train/dfl_loss</td><td></td></tr><tr><td>val/box_loss</td><td></td></tr><tr><td>val/cls_loss</td><td></td></tr><tr><td>val/dfl_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>4e-05</td></tr><tr><td>lr/pg1</td><td>4e-05</td></tr><tr><td>lr/pg2</td><td>4e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.93885</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.60651</td></tr><tr><td>metrics/precision(B)</td><td>0.91058</td></tr><tr><td>metrics/recall(B)</td><td>0.89316</td></tr><tr><td>model/GFLOPs</td><td>8.197</td></tr><tr><td>model/parameters</td><td>3011628</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>1.933</td></tr><tr><td>train/box_loss</td><td>0.53227</td></tr><tr><td>train/cls_loss</td><td>0.26528</td></tr><tr><td>train/dfl_loss</td><td>0.98685</td></tr><tr><td>val/box_loss</td><td>1.40629</td></tr><tr><td>val/cls_loss</td><td>0.63702</td></tr><tr><td>val/dfl_loss</td><td>1.5054</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">yolov8n</strong> at: <a href='https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection/runs/e7d0qg5e' target=\"_blank\">https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection/runs/e7d0qg5e</a><br/> View project at: <a href='https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection' target=\"_blank\">https://wandb.ai/abuzarkhaan580-nc-airsoft/emotion_detection</a><br/>Synced 5 W&B file(s), 24 media file(s), 5 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240830_041051-e7d0qg5e/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"name":"stdout","text":"Ultralytics YOLOv8.2.83  Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 168 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/emotion12121212/valid/labels... 892 images, 0 backgrounds, 0 corrupt: 100%|| 892/892 [00:01<00:00, 857.49it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING 锔 /kaggle/input/emotion12121212/valid/images/images-2020-11-06T004041-046_face_png.rf.356e9608a7ae873d0819895236165983.jpg: 1 duplicate labels removed\n\u001b[34m\u001b[1mval: \u001b[0mWARNING 锔 Cache directory /kaggle/input/emotion12121212/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 56/56 [00:07<00:00,  7.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        892        948      0.909      0.893      0.936      0.607\n                 angry        207        213      0.924      0.892      0.955      0.837\n                 happy        246        283        0.9      0.887      0.915      0.446\n                   sad        232        233      0.892      0.886      0.939      0.564\n             surprised        207        219      0.922      0.909      0.935      0.581\nSpeed: 0.2ms preprocess, 2.4ms inference, 0.0ms loss, 1.1ms postprocess per image\nResults saved to \u001b[1memotion_detection/yolov8n2\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mval()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP@0.5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: 'DetMetrics' object is not subscriptable"],"ename":"TypeError","evalue":"'DetMetrics' object is not subscriptable","output_type":"error"}]},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\n\n# Load the trained YOLOv8 model\nmodel_path = '/kaggle/working/emotion_detection/yolov8n/weights/best.pt'\nmodel = YOLO(model_path)\n\n# Define input and output video paths\ninput_video_path = '/kaggle/input/videotest'\noutput_video_path = '/kaggle/working/'\n\n# Open the input video\ncap = cv2.VideoCapture(input_video_path)\n\n# Get the video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v') # You can use other codecs like 'XVID', 'MJPG'\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Perform inference\n    results = model(frame)\n    \n    # Process results\n    annotated_frame = results.render()[0]\n    \n    # Write the frame to the output video\n    out.write(annotated_frame)\n\n\n\nprint(f\"Processed video saved to {output_video_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T09:33:55.336874Z","iopub.execute_input":"2024-08-29T09:33:55.337788Z","iopub.status.idle":"2024-08-29T09:33:55.403855Z","shell.execute_reply.started":"2024-08-29T09:33:55.337746Z","shell.execute_reply":"2024-08-29T09:33:55.402964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nfrom ultralytics import YOLO\n\n# Load the trained YOLOv8 model\nmodel_path = '/kaggle/working/emotion_detection/yolov8n/weights/best.pt'\nmodel = YOLO(model_path)\n\n# Define input and output video paths\ninput_video_path = '/kaggle/input/videotest/7610395-uhd_4096_2160_25fps.mp4'\noutput_video_path = '/kaggle/working/output_video.mp4'\n\n# Check if file exists\nif not os.path.isfile(input_video_path):\n    raise FileNotFoundError(f\"Input video file not found at: {input_video_path}\")\n\n# Open the input video\ncap = cv2.VideoCapture(input_video_path)\n\n# Check if video capture opened successfully\nif not cap.isOpened():\n    raise ValueError(f\"Error opening video file: {input_video_path}\")\n\n# Get the video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' for MP4 format\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Perform inference\n    results = model(frame)\n\n    # Process results\n    annotated_frame = results.render()[0]\n\n    # Write the frame to the output video\n    out.write(annotated_frame)\n\n# Release everything\ncap.release()\nout.release()\n\nprint(f\"Processed video saved to {output_video_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T09:35:37.485053Z","iopub.execute_input":"2024-08-29T09:35:37.485442Z","iopub.status.idle":"2024-08-29T09:35:37.997856Z","shell.execute_reply.started":"2024-08-29T09:35:37.485399Z","shell.execute_reply":"2024-08-29T09:35:37.996721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport os\n\n# Load the trained YOLOv8 model\nmodel_path = '/kaggle/working/emotion_detection/yolov8n/weights/best.pt'\nmodel = YOLO(model_path)\n\n# Define input and output video paths\ninput_video_path = '/kaggle/input/fsafsdf/JH   Crowd looking at camera v1   7 12 19.mp4'\noutput_video_path = '/kaggle/working/output3.mp4'\n\n# Check if file exists\nif not os.path.isfile(input_video_path):\n    raise FileNotFoundError(f\"Input video file not found at: {input_video_path}\")\n\n# Open the input video\ncap = cv2.VideoCapture(input_video_path)\n\n# Check if video capture opened successfully\nif not cap.isOpened():\n    raise ValueError(f\"Error opening video file: {input_video_path}\")\n\n# Get the video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' for MP4 format\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Perform inference\n    results = model(frame)\n\n    # Iterate through results\n    for result in results:\n        # Plot the results on the frame\n        annotated_frame = result.plot()\n\n        # Write the frame to the output video\n        out.write(annotated_frame)\n\n# Release everything\ncap.release()\nout.release()\n\nprint(f\"Processed video saved to {output_video_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T09:43:31.374493Z","iopub.execute_input":"2024-08-29T09:43:31.375423Z","iopub.status.idle":"2024-08-29T09:43:40.313557Z","shell.execute_reply.started":"2024-08-29T09:43:31.375382Z","shell.execute_reply":"2024-08-29T09:43:40.312744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U -q PyDrive","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:13:16.500142Z","iopub.execute_input":"2024-08-29T10:13:16.500526Z","iopub.status.idle":"2024-08-29T10:13:29.824467Z","shell.execute_reply.started":"2024-08-29T10:13:16.500487Z","shell.execute_reply":"2024-08-29T10:13:29.823226Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Define the source directory and the destination zip file path\nsource_directory = '/kaggle/working/emotion_detection/yolov8n'\nzip_file_path = '/kaggle/working/emotion_detection_yolov8n.zip'\n\n# Create a ZIP archive\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', source_directory)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T06:00:03.615921Z","iopub.execute_input":"2024-08-30T06:00:03.616329Z","iopub.status.idle":"2024-08-30T06:00:04.613752Z","shell.execute_reply.started":"2024-08-30T06:00:03.616286Z","shell.execute_reply":"2024-08-30T06:00:04.612612Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/emotion_detection_yolov8n.zip'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install PyDrive\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\n# Authenticate and create the PyDrive client\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Define the source directory and the destination ZIP file path\nsource_directory = '/kaggle/working/emotion_detection/yolov8n'\nzip_file_path = '/kaggle/working/emotion_detection_yolov8n.zip'\n\n# Create a ZIP archive\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', source_directory)\n\nprint(f\"Directory {source_directory} has been compressed into {zip_file_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T06:02:39.311224Z","iopub.execute_input":"2024-08-30T06:02:39.311610Z","iopub.status.idle":"2024-08-30T06:02:40.254795Z","shell.execute_reply.started":"2024-08-30T06:02:39.311572Z","shell.execute_reply":"2024-08-30T06:02:40.253813Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Directory /kaggle/working/emotion_detection/yolov8n has been compressed into /kaggle/working/emotion_detection_yolov8n.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}